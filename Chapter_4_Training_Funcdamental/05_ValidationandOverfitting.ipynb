{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Let’s revisit what overfitting means. Typically, to get higher accuracy, we build larger and larger models. One consequence is that the model can rote-memorize some or all of the examples. The model learns the examples instead of learning to generalize from the examples to accurately predict examples it never saw during training. In an extreme case, a model could achieve 100% training accuracy yet have random accuracy on the testing (for 10 classes, that would be 10% accuracy)\n",
    "## Validation\n",
    "Let’s say training the model takes several hours. Do you really want to wait until the end of training and then test on the test data to learn whether the model overfitted? Of course not. Instead, we set aside a small portion of the training data, which we call **validation data.**\n",
    "\n",
    "We don’t train the model with the validation data. Instead, after each epoch, we use the validation data to estimate the likely result on the test data.\n",
    "\n",
    "<img src=\"img_4.png\">\n",
    "\n",
    "\n",
    "If a dataset is very small, and using even less data for training has a negative impact, we can use cross-validation. Instead of setting aside at the outset a portion of the training data that the model will never be trained on, a random split is done for each epoch. At the beginning of each epoch, the examples for validation are randomly selected and not used for training for that epoch, and instead used for the validation test.\n",
    "\n",
    "But since the selection is random, some or all of the examples will appear in the training data for other epochs. Today’s datasets are large, so you seldom see the need for this technique. Figure 4.6 illustrates cross-validation splitting\n",
    "\n",
    "<img src=\"img_5.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will train a simple CNN to classify images from the CIFAR-10 dataset. Our dataset is a subset of this dataset of tiny images, of size 32 × 32 × 3. It consists of 60,000 training and 10,000 test images covering 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n",
    "In our simple CNN, we have one convolutional layer of 32 filters with kernel size 3 × 3, followed by a strided max pooling layer. The output is then flattened and passed to the final outputting dense layer. Figure 4.7 illustrates this process.\n",
    "\n",
    "<img src=\"img_6.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Dense, MaxPooling2D, Flatten\n",
    "from keras import Sequential\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 7200)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                72010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72,906\n",
      "Trainable params: 72,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation=\"softmax\")])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['acc'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test = (x_test / 255.0).astype(np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4935 - acc: 0.4758 - val_loss: 1.2806 - val_acc: 0.5616\n",
      "Epoch 2/15\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2352 - acc: 0.5706 - val_loss: 1.1968 - val_acc: 0.5890\n",
      "Epoch 3/15\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1361 - acc: 0.6045 - val_loss: 1.1758 - val_acc: 0.5992\n",
      "Epoch 4/15\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0601 - acc: 0.6349 - val_loss: 1.0880 - val_acc: 0.6244\n",
      "Epoch 5/15\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9999 - acc: 0.6562 - val_loss: 1.0731 - val_acc: 0.6288\n",
      "Epoch 6/15\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9544 - acc: 0.6710 - val_loss: 1.0712 - val_acc: 0.6288\n",
      "Epoch 7/15\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9148 - acc: 0.6882 - val_loss: 1.0318 - val_acc: 0.6496\n",
      "Epoch 8/15\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.8860 - acc: 0.6949 - val_loss: 1.0974 - val_acc: 0.6318\n",
      "Epoch 9/15\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.8582 - acc: 0.7041 - val_loss: 1.0471 - val_acc: 0.6484\n",
      "Epoch 10/15\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.8263 - acc: 0.7157 - val_loss: 1.0682 - val_acc: 0.6334\n",
      "Epoch 11/15\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.8101 - acc: 0.7195 - val_loss: 1.0533 - val_acc: 0.6510\n",
      "Epoch 12/15\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.7876 - acc: 0.7278 - val_loss: 1.0794 - val_acc: 0.6470\n",
      "Epoch 13/15\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.7684 - acc: 0.7331 - val_loss: 1.0576 - val_acc: 0.6542\n",
      "Epoch 14/15\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.7506 - acc: 0.7388 - val_loss: 1.0662 - val_acc: 0.6512\n",
      "Epoch 15/15\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.7308 - acc: 0.7468 - val_loss: 1.0761 - val_acc: 0.6466\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x260707d9f70>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=15, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we’ve added the keyword parameter validation_split=0.1 to the fit() method to set aside 10% of the training data for validation testing after each epoch.\n",
    " The following is the output after running 15 epochs. You can see that after the fourth epoch, the training and evaluation accuracy are essentially the same. But after the fifth epoch, we start to see them spread apart (65% versus 61%). By the 15th epoch, the spread is very large (74% versus 63%). Our model clearly started overfitting around the fifth epoch:\n",
    "\n",
    "<img src=\"img_7.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s now work on getting the model to not overfit to the examples and instead generalize from them. As discussed in earlier chapters, we want to add some regularization—some noise—during training so the model cannot rote-memorize the training examples. In this code example, we modify our model by adding 50% dropout before the final dense layer. Because dropout will slow our learning (because of forgetting), we increase the number of epochs to 20:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5409 - acc: 0.4546 - val_loss: 1.3036 - val_acc: 0.5568\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3073 - acc: 0.5417 - val_loss: 1.2410 - val_acc: 0.5716\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2419 - acc: 0.5686 - val_loss: 1.2078 - val_acc: 0.5814\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2032 - acc: 0.5820 - val_loss: 1.1656 - val_acc: 0.5990\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1679 - acc: 0.5926 - val_loss: 1.1328 - val_acc: 0.6136\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1440 - acc: 0.6056 - val_loss: 1.1606 - val_acc: 0.6058\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1289 - acc: 0.6065 - val_loss: 1.0995 - val_acc: 0.6284\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1147 - acc: 0.6120 - val_loss: 1.1045 - val_acc: 0.6256\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1017 - acc: 0.6175 - val_loss: 1.1139 - val_acc: 0.6230\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0832 - acc: 0.6268 - val_loss: 1.0837 - val_acc: 0.6380\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0785 - acc: 0.6257 - val_loss: 1.0673 - val_acc: 0.6394\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0608 - acc: 0.6341 - val_loss: 1.0528 - val_acc: 0.6448\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0570 - acc: 0.6330 - val_loss: 1.0504 - val_acc: 0.6510\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0535 - acc: 0.6345 - val_loss: 1.0452 - val_acc: 0.6442\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0325 - acc: 0.6398 - val_loss: 1.0773 - val_acc: 0.6332\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0283 - acc: 0.6438 - val_loss: 1.0441 - val_acc: 0.6534\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0280 - acc: 0.6455 - val_loss: 1.0404 - val_acc: 0.6504\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0160 - acc: 0.6515 - val_loss: 1.0295 - val_acc: 0.6504\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0087 - acc: 0.6520 - val_loss: 1.0576 - val_acc: 0.6514\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0071 - acc: 0.6507 - val_loss: 1.0875 - val_acc: 0.6328\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x26078a571c0>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation=\"softmax\")])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=20, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see from the following output that while achieving comparable training accuracy requires more epochs, the training and test accuracy are comparable. Thus, the model is learning to generalize instead of rote-memorizing the training examples:\n",
    "\n",
    "<img src=\"img_8.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss monitoring\n",
    "Up to now, we’ve been focusing on accuracy. The other metric you see outputted is the average loss across batches for both training and valuation data. Ideally, we would like to see a consistent increase in accuracy per epoch. But we might also see sequences of epochs for which the accuracy plateaus or even fluctuates +/– a small amount.\n",
    "\n",
    "What is important is that we see a steady decrease in the loss. The plateau or fluctuations in this case occur because we are near or hovering over lines of linear separation or haven’t fully pushed over a line, but are getting closer as indicated by the decrease in loss.\n",
    "\n",
    " Let’s look at this another way. Assume you’re building a classifier for dogs versus cats. You have two output nodes on the classifier layer: one for cats and one for dogs. Assume that on a specific batch, when the model incorrectly classifies a dog as a cat, the output values (confidence level) are 0.6 for cat and 0.4 for dog. In a subsequent batch, when the model again misclassifies a dog as a cat, the output values are 0.55 (cat) and 0.45 (dog). The values are now closer to the ground truths, and thus the loss is diminishing, but they still have not passed the 0.5 threshold, so the accuracy has not changed yet. Then assume in another subsequent batch, the output values for the dog image are 0.49 (cat) and 0.51 (dog); the loss has further diminished, and because we crossed the 0.5 threshold, the accuracy has gone up\n",
    "\n",
    "## Going deeper with layers\n",
    "\n",
    "As mentioned in earlier chapters, simply going deeper with layers can lead to instability in the model, without addressing the issues with techniques such as identity links and batch normalization. For example, many of the values we are matrix-multiplying are small numbers less than 1. Multiply two numbers less than 1, and you get an even smaller number. At some point, numbers get so small that the hardware can’t represent the value anymore, which is referred to as a vanishing gradient. In other cases, the parameters may be too close to distinguish from each other—or the opposite, spread too far apart, which is referred to as an exploding gradient.\n",
    "\n",
    "The following code example demonstrates this by using a 40-layer DNN absent of methods to protect from numerical instability as we go deeper in layers, such as batch normalization after each dense layer:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_129 (Dense)           (None, 28, 64)            1856      \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_131 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_135 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_136 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_140 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_145 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_146 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_150 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_151 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_155 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_156 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_160 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_161 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_165 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_166 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 28, 64)            4160      \n",
      "                                                                 \n",
      " dense_170 (Dense)           (None, 28, 10)            650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 168,906\n",
      "Trainable params: 168,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(28, 28)))\n",
    "for i in range(40):\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\HOME\\AppData\\Local\\Temp/ipykernel_14384/4284409223.py\", line 5, in <module>\n      model.fit(x_train, y_train, epochs=10, validation_split=0.1)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1862, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5202, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [896,10] and labels shape [32]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_239100]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_14384/4284409223.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mx_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx_train\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;36m255.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mx_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx_test\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;36m255.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_split\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 67\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     68\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 54\u001B[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[0;32m     55\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\HOME\\AppData\\Local\\Temp/ipykernel_14384/4284409223.py\", line 5, in <module>\n      model.fit(x_train, y_train, epochs=10, validation_split=0.1)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1862, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\HOME\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5202, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [896,10] and labels shape [32]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_239100]"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test = (x_test / 255.0).astype(np.float32)\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"img_9.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}