{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Save/ Restore\n",
    "## Save\n",
    "In TF.Keras, we can save both the model and the trained parameters (weights and biases). The model and weights can be saved separately or together. The save()\n",
    "method saves both the weights/biases and the model to a specified folder in TensorFlow SavedModel format. Here is an example\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "X = np.random.random((28,28, 3)).astype(np.float32)\n",
    "Y = 2*X**2 + 10*X  + 10\n",
    "X = tf.convert_to_tensor(X)\n",
    "Y = tf.convert_to_tensor(Y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "from keras import Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 253.7619 - root_mean_squared_error: 15.9299 - val_loss: 246.8778 - val_root_mean_squared_error: 15.7123\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 253.3018 - root_mean_squared_error: 15.9155 - val_loss: 246.4238 - val_root_mean_squared_error: 15.6979\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 252.8359 - root_mean_squared_error: 15.9008 - val_loss: 245.9697 - val_root_mean_squared_error: 15.6834\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 252.3678 - root_mean_squared_error: 15.8861 - val_loss: 245.5160 - val_root_mean_squared_error: 15.6690\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 251.8990 - root_mean_squared_error: 15.8713 - val_loss: 245.0627 - val_root_mean_squared_error: 15.6545\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 251.4303 - root_mean_squared_error: 15.8566 - val_loss: 244.6078 - val_root_mean_squared_error: 15.6399\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 250.9616 - root_mean_squared_error: 15.8418 - val_loss: 244.1528 - val_root_mean_squared_error: 15.6254\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 250.4926 - root_mean_squared_error: 15.8270 - val_loss: 243.6974 - val_root_mean_squared_error: 15.6108\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 250.0231 - root_mean_squared_error: 15.8121 - val_loss: 243.2418 - val_root_mean_squared_error: 15.5962\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 249.5534 - root_mean_squared_error: 15.7973 - val_loss: 242.7854 - val_root_mean_squared_error: 15.5816\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x19db4858ac0>"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_f = Input(shape=(28,3))\n",
    "layer1 = Dense(32, activation=\"relu\")(input_f)\n",
    "output = Dense(1, activation=\"relu\")(layer1)\n",
    "model = Model(input_f, output)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=tf.keras.metrics.RootMeanSquaredError())\n",
    "model.fit(X, Y, epochs=10, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ././11_Model_save/mymodel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"././11_Model_save/mymodel\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "model.save_weights(\"myModelWeights\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Restore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In TF.Keras, we can restore a model architecture and/or the model parameters (weights and biases). Restoring a model architecture is generally done for loading a prebuilt model, while loading both the model architecture and model parameters is generally done for transfer learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "from keras.models import  load_model\n",
    "loaded_model = load_model(\"./11_Model_save/mymodel\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(28, 28, 3), dtype=bool, numpy=\narray([[[False, False, False],\n        [False, False, False],\n        [False, False, False],\n        ...,\n        [False, False, False],\n        [False, False, False],\n        [False, False, False]],\n\n       [[False, False, False],\n        [False, False, False],\n        [False, False, False],\n        ...,\n        [False, False, False],\n        [False, False, False],\n        [False, False, False]],\n\n       [[False, False, False],\n        [False, False, False],\n        [False, False, False],\n        ...,\n        [False, False, False],\n        [False, False, False],\n        [False, False, False]],\n\n       ...,\n\n       [[False, False, False],\n        [False, False, False],\n        [False, False, False],\n        ...,\n        [False, False, False],\n        [False, False, False],\n        [False, False, False]],\n\n       [[False, False, False],\n        [False, False, False],\n        [False, False, False],\n        ...,\n        [False, False, False],\n        [False, False, False],\n        [False, False, False]],\n\n       [[False, False, False],\n        [False, False, False],\n        [False, False, False],\n        ...,\n        [False, False, False],\n        [False, False, False],\n        [False, False, False]]])>"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(X) == Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "1. When a batch of images is fed forward, the difference between the predicted value and the ground truths is the loss. The loss is used by the optimizer to determine how to update the weights on backward propagation.\n",
    "2. A small amount of the dataset is held out, as test data, and not trained on. After training, the test data is used to observe how well the model generalized versus memorizing the data examples.\n",
    "3. Validation data is used after each epoch to detect model overfitting.\n",
    "4. Standardization of the pixel data is preferred over normalization because it contributes to slightly better speed of convergence.\n",
    "5. Convergence occurs when the loss plateaus during training.\n",
    "6. Hyperparameters are used to improve training the model, but are not part of the model.\n",
    "7. Augmentation allows training for invariance with fewer original images.\n",
    "8. Checkpointing is used to recover a good epoch without restarting training after training has diverged.\n",
    "9. Early stopping saves training time and cost by detecting that the model will not improve with further training.\n",
    "10. Small datasets can be trained from in-memory storage and access, but large datasets are trained from on-disk storage and access.\n",
    "11. After training, you save the model architecture and learned parameters and then subsequently restore the model for deployment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}