{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DenseNet\n",
    "The learner component in a DenseNet (https://arxiv.org/abs/1608.06993) consists of four convolutional groups, as shown in figure 5.14. Each group, with the exception for the last group,\n",
    "1. delays pooling to the end of the group, in what is called the transitional block.\n",
    "2. The last convolutional group has no transitional block, since no group  follows.\n",
    "3. The feature maps will be pooled and flattened by the task component, so it is unnecessary (redundant) to pool at the end of the group.\n",
    "4. This pattern of deferring final pooling in the last group to the task component continues to be a common convention today.\n",
    "\n",
    "\n",
    "<img src=\"img_6.png\"/>\n",
    "\n",
    "A DenseNet block is essentially a residual block, except that in place of adding (matrix add operation) the identity link to the output, it is concatenated. In a ResNet, the\n",
    "information from previous inputs is passed only one block forward. Using concatenation, the information from the feature maps accumulates, and each block passes all\n",
    "the accumulative information forward to all subsequent blocks.\n",
    "\n",
    "This concatenation of feature maps would result in a continued growth in size of feature maps and corresponding parameters as we go deeper in layers. To control (reduce) the growth, the transitional block at the end of each convolutional block compresses (reduces) the size of the concatenated feature maps. Otherwise, without the reduction, the number of parameters to learn would grow substantially as we grow deeper, resulting in taking longer to train without a benefit in increased accuracy.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following is an example implementation of using the skeleton template for coding the learner component of a DenseNet.\n",
    " Note that we pop off the last group configuration attributes before iterating through the groups. We treat the last group as a special case, as the group does not end in a transition block.\n",
    " Alternatively, we could have used a configuration parameter to indicate whether or not a group contains a transition block, eliminating the special case (that is, coding a separate block construction). The parameter reduction specifies the amount of feature map size reduction during delayed pooling:\n",
    "\n",
    "<img src=\"img_7.png\" />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Letâ€™s look at a convolutional group in a DenseNet (figure 5.15). It consists of only two types of convolutional blocks.\n",
    "1. The first blocks are DenseNet blocks for feature learning, and\n",
    "2. the last block is a transitional block for reducing the size of the feature maps prior to the next group, which is referred to as the **compression factor**.\n",
    "\n",
    "<img src=\"img_8.png\"/>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A DenseNet block is essentially a residual block, except that in place of adding (matrix add operation) the identity link to the output, it is concatenated. In a ResNet, the\n",
    "information from previous inputs is passed only one block forward. Using concatenation, the information from the feature maps accumulates, and each block passes all the accumulative information forward to all subsequent blocks.\n",
    "\n",
    "This concatenation of feature maps would result in a continued growth in size of feature maps and corresponding parameters as we go deeper in layers. To control (reduce) the growth, the transitional block at the end of each convolutional block compresses (reduces) the size of the concatenated feature maps. Otherwise, without the reduction, the number of parameters to learn would grow substantially as we grow deeper, resulting in taking longer to train without a benefit in increased accuracy.\n",
    "\n",
    "\n",
    "<img src=\"img_9.png\"/>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}