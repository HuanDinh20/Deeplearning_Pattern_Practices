{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. CNN\n",
    "Early convolutional neural networks are a type of neural network that can be viewed as consisting of two parts, a frontend and a backend.\n",
    "\n",
    "The name convolutional neural network comes from the frontend, referred to as a convolutional layer. The frontend acts as a preprocessor.\n",
    "\n",
    "The DNN backend does the **classification learning**. The CNN frontend preprocesses the image data into a form that is computationally practical for the DNN to learn from. The CNN frontend does the **feature learning**.\n",
    "\n",
    "Figure 3.1 depicts a CNN in which the convolutional layers act as a frontend for learning features from the image, which is then passed to a backend DNN for classification from the features.\n",
    "\n",
    "<img src=\"img.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Why we use a CNN over a DNN for image models\n",
    "\n",
    "Once we get to larger image sizes, the number of pixels for a DNN becomes computationally too expensive to be feasible.\n",
    "\n",
    "## Down-sampling (Resizing)\n",
    "To solve the problem of having too many parameters, one approach is to reduce the resolution of the image through a process called down-sampling. But if we reduce the image resolution too far, at some point we may lose the ability to clearly distinguish what’s in the image; it becomes fuzzy and/or has artifacts. So, the first step is to *reduce the resolution down to a level that we still have enough details*.\n",
    "\n",
    "\n",
    "A common convention for everyday computer vision is around 224 × 224 pixels. We do this by resizing. Even at this lower resolution and three channels for color\n",
    "images, and an input layer of 1024 nodes, we still have 154 million weights to update and learn (224 × 224 × 3 × 1024); see figure 3.2.\n",
    "\n",
    "<img src=\"img_1.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "But then we shift the cat to the middle of the picture. Now we have to learn to recognize a cat from a new set of pixel positions—wah! Now move it to the\n",
    "right, add the cat lying down, jumping in the air, and so forth.   Learning to recognize an image from various perspectives is referred to as **translational invariance**.\n",
    "\n",
    "##  Feature detection\n",
    "In a computer, a convolutional layer performs the task of feature detection within an image. Each convolution consists of a set of filters. These filters are N × M matrices of values that are used to detect the likely presence of a feature.\n",
    "\n",
    "<img src=\"img_2.png\" width=1024>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Additionally, we specify a stride, which is the rate that the filter is slid across the image. Each filter that is learned produces a feature map, which is a mapping that indicates how strongly the feature is detected at a particular location in the image, as depicted in figure 3.4.\n",
    "\n",
    "<img src=\"img_3.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The filter can either stop when it gets to the edge of the image, or continue until the last column is covered, as shown in figure 3.5. The former case is called no padding. The latter case is called padding. When the filter goes partially over the edge, we want to give a value for these imaginary pixels. Typical values are zero or same—the same as the last column.\n",
    "\n",
    "<img src=\"img_4.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When you have multiple convolutional layers, a common practice is to either keep the same number or increase the number of filters on deeper layers, and to use a stride of 1 on the first layer and 2 on deeper layers. The increase in filters provides the means to go from coarse detection of features to more detailed detection within coarse features. The increase in stride offsets the increase in the size of retained data; this process is referred to as feature pooling, where the feature maps are down-sampled.\n",
    "\n",
    "CNNs use two types of down-sampling: pooling and feature pooling. In pooling, a fixed algorithm is used to downsample the size of the image data. In feature pooling, the best down-sampling algorithm for the specific dataset is learned:\n",
    "More filters => More data\n",
    "Bigger strides => Less data\n",
    "\n",
    "## Pooling\n",
    "Even though each feature map that’s generated is typically equal in size to or smaller than the image, the total amount of data increases because we generate multiple feature maps.\n",
    "\n",
    "The next step is to reduce the total amount of data, while retaining the features detected and corresponding spatial relationships among the detected features.\n",
    "\n",
    "As I’ve said, this step is referred to as pooling, which is the same as downsampling (or subsampling). In this process, the feature maps are resized to a smaller dimension using either the max (downsampling) or mean pixel average (subsampling) within the feature map. In pooling, as depicted in figure 3.6, we set the size of the area to pool as an N × M matrix as well as a stride. The common practice is a 2 × 2 pool size with a stride of 2. This will result in a 75% reduction in pixel data, while still preserving enough resolution that the detected features are not lost.\n",
    "\n",
    "<img src=\"img_5.png\">\n",
    "\n",
    "Another way to look at pooling is in the context of information gain. By reducing unwanted or less-informative pixels (for example, from the background) we are\n",
    "reducing entropy and making the remaining pixels more informative.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flattening\n",
    "Recall that deep neural networks take vectors as input—one-dimensional arrays of numbers. In the case of the pooled maps, we have a list (plurality) of 2D matrices, so we need to transform them into a single 1D vector, which then becomes the input vector to the DNN. This process is called flattening: we flatten the list of 2D matrices into a single 1D vector.\n",
    "\n",
    "It’s pretty straightforward. We start with the first row of the first pooled map as the beginning of the 1D vector. We then take the second row and append it to the end, and then the third row, and so forth. We then proceed to the second pooled map and do the same, continuously appending each row until we’ve completed the last pooled map. As long as we follow the same sequencing through pooled maps, the spatial relationships among detected features will be maintained across images for training and inference (prediction), as depicted in figure 3.7.\n",
    "\n",
    "<img src=\"img_6.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}