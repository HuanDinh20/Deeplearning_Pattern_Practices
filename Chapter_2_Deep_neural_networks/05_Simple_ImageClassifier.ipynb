{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Simple image classifier\n",
    "## Flattening\n",
    "Flattening is the process of placing each row in sequential order into a vector.\n",
    "\n",
    "<img src=\"img_7.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the input shape to the Flatten object is the 2D shape (28, 28). The output from this object will be a 1D shape of (784,):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Activation, Dropout, ReLU\n",
    "from keras import Sequential"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequential_model = Sequential([\n",
    "    Flatten(input_shape=(28,28)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "sequential_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "sequential_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Overfitting and dropout\n",
    "\n",
    "During training, a dataset is split into training data and test data (also known as hold-out data). Only the training data is used during the training of the neural network. Once the neural network has reached convergence, as shown in figure 2.10.\n",
    "\n",
    "<img src=\"img_8.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Afterward, to obtain the accuracy of the model on the training data, the training data is forward-fed again without backward propagation enabled, so there is no learning. This is also known as running the trained neural network in inference or prediction mode.\n",
    "\n",
    "In a train/test split, the test data, which has been set aside and not used as part of training, is forward-fed again without backward propagation enabled to obtain an accuracy.\n",
    "\n",
    " Once you reach convergence, continually passing the training data through the neural network will cause the neurons to more and more memorize the training samples versus generalizing to samples that are never seen during training. This is known as overfitting. When the neural network is overfitted to the training data, you will get high training accuracy, but substantially lower accuracy on the test/evaluation data.\n",
    "\n",
    "To address overfitting when training neural networks, we can use regularization. This adds small amounts of random noise during training to prevent the model from memorizing samples and better generalize to unseen samples after the model is trained.\n",
    "\n",
    "The most basic type of regularization is called dropout. Dropout is like forgetting. When we teach young children, we use rote memorization, as when we ask them to memorize the multiplication table of numbers 1 through 12. We have them iterate, iterate, iterate, until they can recite in any order the correct answer 100% of the time. But if we ask them “What is 13 times 13?” they will likely give us a blank look. At this point, the times table is overfitted in their memory. The answer to each multiplication pair, the samples, is hardwired in the brain’s memory cells, and they don’t have a way to transfer that knowledge beyond 1 through 12\n",
    "\n",
    "As children get older, we switch to abstraction. Instead of teaching a child to memorize the answers, we teach them how to compute the answer—albeit they may make computation mistakes. During this second teaching phase, some neurons related to the rote memorization will die. The combination of the death of those neurons (which means forgetting) and abstraction allows the child’s brain to generalize and now solve arbitrary multiplication problems, though at times they will make a mistake, even at times in the 12 × 12 times table, with some probabilistic distribution.\n",
    "\n",
    "The dropout technique in neural networks mimics this process of moving to abstraction, and learning, with probabilistic distribution of uncertainty. Between any\n",
    "layer, you can add a dropout layer, where you specify a percentage (between 0 and 1) to forget. The nodes themselves won’t be dropped, but instead a random selection on each forward feed during training will not pass a signal forward. The signal from the randomly selected node will be forgotten. So, for example, if you specify a dropout of 50% (0.5), on each forward feed of data, a random selection of half of the nodes will not send a signal.\n",
    "\n",
    "The advantage here is that we minimize the effect of localized overfitting while continuously training the neural network for overall convergence. A common practice for dropout is setting values between 20% and 50%."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28,28)),\n",
    "    Dense(512),\n",
    "    Dropout(0.5),\n",
    "    ReLU(),\n",
    "    Dense(512),\n",
    "    Dropout(0.5),\n",
    "    ReLU(),\n",
    "    Dense(10),\n",
    "    Activation(\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}