{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural network basics\n",
    "1. input layer to a neural network\n",
    "2. how this is connected to an output layer\n",
    "3. how hidden layers are added in between\n",
    "4. how the layers are made of nodes\n",
    "5. what nodes do\n",
    "6. how layers are connected to each other to form fully connected neural networks\n",
    "\n",
    "## 1. Input layer\n",
    "The input layer to a neural network takes numbers. All the input data is converted to numbers. Everything is a number.\n",
    "\n",
    "Neural networks take numbers as vectors, matrices, or tensors.\n",
    "\n",
    "Speaking of numbers, you might have heard terms like **normalization or standardization**. In **standardization**, numbers are converted to be **centered around a\n",
    "mean of zero**, with **one standard deviation** on **each side** of the **mean**.\n",
    "\n",
    "Standardization is basically a button to push, and it doesn’t even need a lever, so there are no parameters to set.\n",
    "\n",
    "With TensorFlow 2.0, Keras is built in and the recommended model API, referred to now as TF.Keras. TF.Keras is based on object-oriented programming with a collection of classes and associated methods and properties.\n",
    "\n",
    "Let’s start simply. Say we have a dataset of housing data. Each row has 14 columns\n",
    "of data. One column indicates the sale price of a home. We are going to call that the\n",
    "label. The other 13 columns have information about the house, such as the square\n",
    "footage and property tax. It’s all numbers. We are going to call those the features. What\n",
    "we want to do is learn to predict (or estimate) the label from the features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Input"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<KerasTensor: shape=(None, 13) dtype=float32 (created by layer 'input_1')>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input(shape=(13, ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " We will start by first importing the Keras module from TensorFlow, and then\n",
    "instantiate an Input class object. For this class object, we define the shape or dimen\u0002sions of the input. In our example, the input is a one-dimensional array (a vector) of 13 elements, one for each feature\n",
    "\n",
    "This output shows you what Input(shape=(13,)) evaluates to. It produces a tensor object named input_1.\n",
    "\n",
    "This name will be useful later in assisting you in debugging your models. The `None` in shape shows that the input object takes an unbounded number.\n",
    "of entries (examples or rows) of 13 elements each.\n",
    "\n",
    "That is, at runtime it will bind the number of one-dimensional vectors of 13 elements to the actual number of examples (rows) you pass in, referred to as the **(mini) batch size**. The dtype shows the default data type of the elements, which in this case is a 32-bit float (single precision).\n",
    "\n",
    "## 2 Deep neural networks\n",
    "The neural network has one or more layers between the input layer and the output layer == Deep neural network.\n",
    "\n",
    "Fully connected neural network (FCNN): every node on each layer is connected to every other node on the next layer\n",
    "\n",
    "<img src=\"img.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Feed-forward networks\n",
    "The DNN and CNN known as feed forward neural networks.\n",
    "\n",
    "Feed-forward means that data moves through the network sequentially, in one direction, from the input layer to the output layer.\n",
    "\n",
    "The inputs are passed as parameters in the input layer, and the function performs a sequenced set of actions based on the\n",
    "inputs (in the hidden layers) and then outputs a result (the output layer).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Sequential API Methods\n",
    "The sequential API method is easier to read and follow for beginners, but the tradeoff is\n",
    "that it is less flexible.\n",
    "\n",
    "Essentially, you create an empty feed-forward neural network with the Sequential class object, and then “add” one layer at a time, until the output\n",
    "layer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras import layers\n",
    "model = Sequential(name=\"my_sequential\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model.add(layers.Dense(2, activation=\"relu\", name=\"layer1\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model.add(layers.Dense(3, activation=\"relu\", name=\"layer2\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternatively, the layers can be specified in sequential order as a list passed as a parameter when instantiating the Sequential class object"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model2 = Sequential(\n",
    "[    layers.Dense(2, activation=\"relu\", name=\"layer1\"),\n",
    "    layers.Dense(3, activation=\"relu\", name=\"layer2\")]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<keras.engine.sequential.Sequential at 0x17d633143d0>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<keras.engine.sequential.Sequential at 0x17d6332c4c0>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "if I am writing code for production, I use the sparser list method, where I can visualize and edit the code more easily."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (3, 2)                    8         \n",
      "                                                                 \n",
      " layer2 (Dense)              (3, 3)                    9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17\n",
      "Trainable params: 17\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.build(input_shape=((3,)))\n",
    "model2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (3, 2)                    8         \n",
      "                                                                 \n",
      " layer2 (Dense)              (3, 3)                    9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17\n",
      "Trainable params: 17\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(3,3))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Functional API method\n",
    "The functional API method is more advanced, allowing you to construct models that are nonsequential in flow—such as branches, skip links, and multiple inputs and outputs.\n",
    "\n",
    "You build the layers separately and then tie them together. This latter step gives you the freedom to connect layers in creative ways.\n",
    "\n",
    "\n",
    "Essentially, for a feed-forward neural network, you create the layers, bind them to another layer or layers, and then pull all the layers together in\n",
    "a final instantiation of a Model class object."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from keras import Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "input = Input((28,28,1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "x = layers.Conv2D(filters= 16, kernel_size=3, activation=\"relu\")(input)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "model = Model(input, encoder_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 16)          4624      \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 16)               0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,672\n",
      "Trainable params: 18,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"img_1.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model  = Input + Output\n",
    "Steps:\n",
    "1. Construct input\n",
    "2. Construct hidden layers. **put input to hidden layers**\n",
    "3. Construct output\n",
    "model = Model(intput, output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  6. Input shape vs. input layer\n",
    "The input shape and input layer can be confusing at first. They are not the same thing.\n",
    "\n",
    "More specifically, the number of nodes in the input layer does not need to match the shape of the input vector. That’s because every element in the input vector will be passed to every node in the input layer\n",
    "<img src=\"img_2.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each connection between an element in the input vector and a node in the input layer has a weight, and each node in the input layer has a bias.\n",
    "\n",
    "The weights and biases are what the neural network will “learn” during training. The weights and biases are also referred to as parameters. These values stay with the model after it is trained. This operation will otherwise be invisible to you."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Dense Layers\n",
    "In TF.Keras, layers in an FCNN are called dense layers. A dense layer has n number of nodes and is fully connected to the previous layer.\n",
    "\n",
    "In this example, we are going to use a neural network as a regressor, which means the neural network will output a single\n",
    "real number:\n",
    "Input layer = 10 nodes\n",
    "Hidden layer = 10 nodes\n",
    "Output layer = 1 node\n",
    "*7.1 Sequential API*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 10)                140       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261\n",
      "Trainable params: 261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(10, input_shape=(13,)),\n",
    "    layers.Dense(10),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Functional API*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "input_f = Input((13,))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "x = layers.Dense(10)(input_f)\n",
    "hidden_layers = layers.Dense(10)(x)\n",
    "outputs = layers.Dense(1)(hidden_layers)\n",
    "model = Model(input_f, outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 13)]              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                140       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261\n",
      "Trainable params: 261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"img_3.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation functions\n",
    "When training or predicting (via inference), each node in a layer will output a value to the\n",
    "nodes in the next layer. We don’t want to pass the value as-is, but instead sometimes want\n",
    "to change the value in a particular manner. This process is called an activation function.\n",
    "\n",
    "Think of a function that returns a result, like return result. In the case of an acti\u0002vation function, instead of returning result, we would return the result of passing the\n",
    "result value to another (activation) function, like return A(result), where A() is the\n",
    "activation function. Conceptually, you can think of this as follows:\n",
    "\n",
    "def layer(params):\n",
    "     \"\"\" inside are the nodes \"\"\"\n",
    "     result = some_calculations\n",
    "     return A(result)\n",
    "def A(result):\n",
    "     \"\"\" modifies the result \"\"\"\n",
    "     return some_modified_value_of_result\n",
    "\n",
    "Activation functions assist neural networks in learning faster and better. By default, when no activation function is specified, the values from one layer are passed as-is (unchanged) to the next layer. The most basic activation function is a step function. If the value is greater than 0, a 1 is outputted; otherwise, a 0 is outputted. The step function hasn’t been used in a long, long time.\n",
    "\n",
    "Activation functions assist in finding the nonlinear separations and corresponding clustering of nodes within input sequences, which then learn the (near) linear relationship to the output. Most of the time, you will use three activation functions: the rectified linear unit (ReLU), sigmoid, and softmax.\n",
    "\n",
    "The ReLU is generally used between layers. While early researchers used different activation functions (such as a hyperbolic tangent) between layers, researchers found that the ReLU produced the best result in training a model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 28, 28, 10)        20        \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 28, 28, 10)        0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 28, 28, 10)        110       \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 28, 28, 10)        0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 28, 28, 1)         11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 141\n",
      "Trainable params: 141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_f = Input((28,28,1))\n",
    "X = layers.Dense(10)(input_f)\n",
    "X = layers.ReLU()(X)\n",
    "X = layers.Dense(10)(X)\n",
    "X = layers.ReLU()(X)\n",
    "X = layers.Dense(1)(X)\n",
    "model = Model(input_f, X)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shorthand Syntax\n",
    "TF.Keras provides a shorthand syntax when specifying layers. You don’t actually need\n",
    "to separately specify activation functions between layers, as we did in the previous\n",
    "example. Instead, you can specify the activation function as a (keyword) parameter\n",
    "when instantiating a Dense layer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 28, 28, 10)        20        \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 28, 28, 10)        110       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 28, 28, 1)         11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 141\n",
      "Trainable params: 141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(10, input_shape=(28, 28, 1), activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"relu\"),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Improving accuracy with an optimizer\n",
    "\n",
    "Once you’ve completed building the feed-forward portion of your neural network,  you need to add a few things for training the model. This is done with the compile() method. This step adds the backward propagation during training.\n",
    "\n",
    " Each time we send data (or a batch of data) forward through the neural network, it calculates the errors in the predicted results (known as the loss) from the actual values (called labels) and uses that information to incrementally adjust the weights and biases of the nodes. This, for a model, is the process of learning.\n",
    "\n",
    "The calculation of the error, as I’ve said, is called a loss. It can be calculated in many ways. Since we designed our example neural network to be a regressor (meaning that the output, house price, is a real value), we want to use a loss function that is best suited for a regressor. Generally, for this type of neural network, we use the mean square error method of calculating a loss. In Keras, the compile() method takes a (keyword) parameter loss used to specify how we want to calculate the loss. We are going to pass it the value mse (for mean square error).\n",
    "\n",
    "For our regressor neural network, we will use the rmsprop method (root mean square\n",
    "property):\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}