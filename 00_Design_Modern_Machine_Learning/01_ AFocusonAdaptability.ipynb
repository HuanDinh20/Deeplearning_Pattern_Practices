{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # A focus on adaptability\n",
    "Because this book is aimed at less-experienced deep learning engineers and data scientists, part 1 starts with the designs of the basic deep neural networks (DNNs), convolutional neural networks (CNNs), and residual neural networks (ResNets). Part 1 also looks at the architecture of simple training pipelines. Whole books are written about just these networks and architectures, so here you’ll get more of a reminder of how they work, with an emphasis on design patterns and principles\n",
    "\n",
    "That said, if you are well versed in the fundamentals, you can go directly to part 2, which looks at the seminal models in the development of deep learning. My approach is to provide enough information about each model design so that you can play around with them and come up with solutions to the AI challenges you may encounter. The models are introduced more or less chronologically, so part 2 also serves as a kind of history of deep learning, with an emphasis on the evolution from one model to the next.\n",
    "\n",
    "Now, if enterprise production is moving toward automatic learning for model\n",
    "development, you may wonder about the value of examining these manually\n",
    "designed, formerly state-of-the-art (SOTA) models. Many of these models, however,\n",
    "continue to be used as stock models, particularly for transfer learning. Some of the\n",
    "others never made it into production at all, but were responsible for discoveries that\n",
    "continue to be used today.\n",
    "\n",
    "Model development for production continues to be a combination of automatic and hand-designed learning—which is often crucial for proprietary needs or advantages. But designing by hand does not mean starting from scratch; typically, you would start with a stock model and make tweaks and adjustments. To do this effectively, you need to know how the model works and why it works that way, the concepts that underlie its design, and the pros and cons of alternative building blocks you will learn from other SOTA models.\n",
    "\n",
    "The final part of the book takes a deep dive into the design patterns for training\n",
    "and deployment for production. While not all readers will be deploying the kinds of\n",
    "enterprise systems that are my focus, I feel this information is relevant to all. Becoming familiar with many types—and sizes—of systems addressing a variety of problems\n",
    "can help you when you need to think outside the box to solve a problem. The more you know about the underlying concepts and designs, the more able and adaptable you become\n",
    "\n",
    "This adaptability is probably the most valuable takeaway from this book. Production involves a vast number of moving parts, and an endless flow of “monkey wrenches” being tossed into the mix. If engineers or data scientists simply rote-memorize sets of reproducible steps in a framework, how will they handle the diversity of tasks they’ll encounter and resolve the monkey wrenches thrown at them? Employers look for more than just skill and experience; they want to know how technically adaptive you are.\n",
    "\n",
    "Imagine yourself in an interview: you score high on skill and work experience and nail the stock machine learning (ML) coding challenge. Then the interviewers throw you a monkey wrench, an unexpected or unusual problem. They do this to observe how you think through a challenge, which concepts you apply and the reasoning behind them, how you evaluate the pros and cons of the various solutions, and your ability to debug. That’s adaptability. And that’s what I hope deep learning developers and data scientists will get from this book\n",
    "## 1.1 Computer vision leading the way\n",
    "I teach all of these concepts primarily in the context of computer vision, because design patterns evolved first in computer vision. But they are applicable to natural-language processing (NLP), structured data, signal processing, and other fields. If we roll the clock back to prior to 2012, ML in all fields was mostly using classical statistics-based methods.\n",
    "\n",
    "Various academic researchers, such as Fei-Fei Liu at Stanford University and Geoffrey Hinton of the University of Toronto, began to pioneer applying neural networks to computer vision. Liu, along with her students, compiled a computer vision dataset, now known as ImageNet, to advance the research into computer vision. ImageNet, along with the PASCAL dataset, became the basis for the annual ImageNet Large\n",
    "Scale Vision Recognition Challenge (ILSVRC) competition in 2010. Early entries used traditional image recognition/signal processing methods.\n",
    "\n",
    "Then, in 2012, Alex Krizhevsky, also of the University of Toronto, entered a deep learning model, AlexNet, using convolution layers. This model won the ILSVRC contest and by a sizable margin. The AlexNet model, jointly designed with Hinton and Ilya Sutskever, kicked off deep learning. In their corresponding paper, “ImageNet Classification with Deep Convolutional Neural Networks” they showed how neural networks can be designed.\n",
    "\n",
    "In 2013, Matthew Zeiler and Rob Fergus of New York University won the competition\n",
    "by fine-tuning AlexNet into what they called ZFNet. This pattern of building on each\n",
    "other’s success continued. The Visual Geometry Group at Oxford expanded on the\n",
    "AlexNet design principles and won the 2014 contest. In 2015, Kaiming He and others\n",
    "at Microsoft Research further expanded on the AlexNet/VGG design principles and\n",
    "introduced new design patterns, winning the competition. Their model, ResNet, and\n",
    "their “Deep Residual Learning for Image Recognition”, set off a surge in discovering and exploring the design space of CNNs.\n",
    "## 1.2  The evolution in machine learning approaches\n",
    " In these early years of developing design principles and design patterns using deep learning for computer vision, developments in natural-language understanding (NLU) and structured data models lagged behind and continued to focus on classical approaches.\n",
    "\n",
    "They used classical ML frameworks, like the Natural Language Toolkit (NLTK) for text input, and classical algorithms based on decision trees, like random forest, for structured data input.\n",
    "\n",
    " In the NLU field, progress was made with the introduction of RNNs and longshort-term-memory (LSTM) and gated recurrent unit (GRU) layers. That progress took a leap in 2017 with the introduction of the Transformer design pattern for natural language, and the corresponding paper “Attention Is All You Need”\n",
    "\n",
    "Google Brain, a deep learning research organization within Google AI, adopted early a comparable attention mechanism in ResNet.\n",
    "\n",
    "Likewise, advancements in design patterns for structured data evolved with the introduction of the wide-and-deep model pattern, outlined in “Wide & Deep Learning for Recommender Systems”\n",
    "\n",
    "\n",
    "# The evolution in machine learning approaches\n",
    "\n",
    "To understand the modern approach, we first have to understand where we are with AI and ML, and how we got here. This section presents several top-level approaches and design patterns for working in today’s production environment, including intelligent automation, machine design, model fusion, and model amalgamation.\n",
    "\n",
    "##  Classical AI vs. narrow AI\n",
    "In classical AI (also known as semantic AI), models were designed as rule-based systems.\n",
    "\n",
    "These systems were used to solve problems that could not be solved by a mathematical equation. Instead, the system was set up to mimic a subject matter or domain expert.\n",
    "\n",
    "<img src=\"img.png\" >\n",
    "\n",
    "*Figure 1.1 offers a visual of this approach*\n",
    "\n",
    "** Classical AI**\n",
    " 1. worked well in input spaces that were low-dimensionality (as in, had a low number of distinct inputs);\n",
    " 2. had an input space that could be broken into discrete segments, such as categories or bins; and maintained a **strong linear relationship** between the discrete space and the output.\n",
    " 3. The domain expert designed a set of rules, based on inputs and state transformations, that mimicked their expertise.\n",
    " 4. A programer then converted these rules into a rule-based system, typically of the form “If A and B are true, then C is true.”\n",
    "\n",
    "Systems like this were well suited for problems like predicting the quality and appropriateness of a wine, which required only a small set of rules.\n",
    "\n",
    " **In narrow AI** (also known as statistical AI),\n",
    " 1. a model gets trained on a large amount of data,\n",
    " 2. alleviating the need for domain experts. Instead, the model uses principles of statistics to learn patterns in the distribution of the input data, also referred to as a sampling distribution.\n",
    " 3. These patterns can then be applied with high accuracy to samples not seen in training.\n",
    " 4. When trained with a sampling distribution made up of large amounts of data that is representative of the larger population, or population distribution, we can model problems without the constraints that come with classical AI. In other words,\n",
    " 5. narrow AI can work very well with substantially higher dimensionality in the input space (meaning a large number of distinct inputs), and with inputs that can be a mix of discrete and continuous.\n",
    "\n",
    "#### Comparison\n",
    "Let’s contrast **rule-based** with narrow AI by applying both to predicting the sale price of a house. A rule-based system could generally consider only a small number of inputs; for instance, lot size, square footage, number of bedrooms, number of bathrooms, and property tax. A system like this could predict a median price for comparable homes, but not for any one home, because of nonlinearity in the relationships of\n",
    "the property to the price.\n",
    "\n",
    "1. In a linear relationship,the value of one variable predicts the value of another. For example, say we have the function y = f(x), which we define as 2 × x. The value of y can be predicted with 100% confidence for any value x.\n",
    "2. In a nonlinear relationship, the value of y can be predicted only with a probability distribution on any\n",
    "value x.\n",
    "\n",
    "In **narrow AI**, we substantially increase the number of inputs to learn the nonlin\u0002earity, such as adding the year the house was built, when permits for upgrades were granted, the type of architecture, materials used for roofing and siding, school district information, employment opportunities, average income, the neighborhood, as well as crime, and vicinity to parks, public transportation, and highways. These additional variables help the model learn the probability distribution with high confidence. Inputs whose value is from a fixed set, such as building architecture, are discrete, while inputs that are from an unbounded range, like average income, are continuous.\n",
    "\n",
    "\n",
    " Narrow AI models work well with inputs that have a high level of non-linearity to the outputs (predictions), by learning the boundaries to segment the inputs—again, if those segments have a strongly linear relationship to the output. These types of models are based on statistics, requiring large amounts of data, and are called narrow AI because they are good at solving narrow problems consisting of a limited range of tasks within one field. Narrow models are not so good at generalizing to problems of a wide scope.\n",
    "\n",
    "\n",
    "<img src=\"img_1.png\">\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another way to see the difference between classical AI and narrow AI is by looking at the error-rate reduction in both kinds of models, as deep learning is continuously pushing to the Bayes theoretical error limit. Bayes described this theoretical error limit as a progression, as shown in figure 1.3.\n",
    "\n",
    "<img src=\"img_2.png\" >"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " First, what would be the error rate of an average non-expert solving a task? Then, what would be the error rate of an expert solving the task (this is analogous to semantic AI)? What would be the error rate with a roomful of experts solving the task? And finally, the theoretical limit: what would be the error rate of an infinite number of experts solving the task?\n",
    "\n",
    " Deep learning in vast numbers of computer vision and NLP tasks has achieved the error rate of a roomful of experts, vastly outperforming both conventional software applications and expert systems. In 2020, researchers and enterprise ML engineers began pursuing production systems that are in the realm of the Bayes theoretical error limit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Next steps in computer learning\n",
    "\n",
    "Now that we understand how we got here, where, exactly, are we?\n",
    "\n",
    "As computer learning has changed,\n",
    "1. we first moved from artificial intelligence to intelligent automation.\n",
    "2. And then we’ve moved into machine design, model fusion, and model amalgamation.\n",
    "\n",
    "#### INTELLIGENT AUTOMATION\n",
    "\n",
    "Early artificial intelligence meant classical AI, which was mostly rule-based and required domain experts.\n",
    "\n",
    "Then, in narrow AI, we applied statistics to learning, eliminating the need for a domain expert.\n",
    "\n",
    "The next major advance was **intelligent automation (IA)**\n",
    "* In this approach, the models learn a (near) optimal way to automate the process, exceeding the performance and accuracy when compared to the manual or computer-automated counterpart\n",
    "\n",
    "Typically, the IA system works as a pipeline process.\n",
    "    * Cumulative information, transformations, and state transitions are the inputs to a model at various points in the pipeline.\n",
    "    * The output, or prediction, of each model is used to perform the next information transformation and/or decide the next state transition\n",
    "    * Typically, each model is trained and deployed independently, usually as a microservice, with a backend application driving the whole pipeline process.\n",
    "\n",
    "That’s intelligent automation. Instead of programmers coding a predesigned algorithm to automate, the programmers guide the model to intelligently learn the optimal algorithm. Figure 1.4 maps out how we would apply IA to a single step in a claim processing pipel\n",
    "\n",
    "<img src=\"img_3.png\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The IA processes downstream continue to reduce/eliminate human operator costs and further reduce error rates. Once we get to the final step, a trained subject-matter expert (SME) makes the final review for authorization (or nonauthorization) of payment. Now that the information that the SME reviews is of higher accuracy, the human subjective decision is more reliable, further reducing costs of making the wrong subjective decision.\n",
    "\n",
    "We in the industry have stopped using the term machine learning and replaced it with machine design, to make the analogy to computer-aided design (CAD). We applied CAD to problems that were too complex to engineer even a suboptimal solution. These systems had building components, mathematical knowledge, and expert system rules, and the SMEs guided the CAD systems to find a good suboptimal solution.\n",
    "\n",
    "In machine design, the system instead learns the building components, the mathematical knowledge, and the rules, and the ML engineer guides the machine design to find the optimal solution.\n",
    "\n",
    "By moving to machine design, we free up the high-value people assets to work on the next level of challenging problems, accelerating their technical progression and bringing a higher return on investment (ROI) per staffer for the business.\n",
    "\n",
    "#### MACHINE DESIGN\n",
    "Before deep learning, SMEs designed software programs to search for good solutions in parts of the software and hardware that had high complexity. Typically, these programs were a combination of search optimization and rule-based techniques.\n",
    "\n",
    "In the next advancement, machine design, **the models learn a (near) optimal way to design and integrate the software and hardware components**. These systems exceed in performance, accuracy, and complexity, even when compared to models designed by an SME with the assistance of a CAD program. The human designer uses their realworld expertise to guide the model’s search space for solutions.\n",
    "\n",
    "In machine design, in addition to training a model, the system learns the optimal training pipeline for an adversarial model, known as the surrogate.\n",
    "\n",
    "#### MODEL FUSION\n",
    "Model fusion is the next advancement in developing more-accurate, lower-cost systems for predictive maintenance and fault detection, such as those used in IoT sensor systems.\n",
    "\n",
    "The problem with these traditional systems has been that they are subject to high environmental variance that affects their reliability.\n",
    "\n",
    "#### MODEL AMALGAMATION\n",
    "In model amalgamation, the model(s) essentially become the entire application, which directly shares model components and outputs and learns a communication interface between the models. All this happens without the need for a bulky backend application or microservices\n",
    "\n",
    "<img src=\"img_4.png\">\n",
    "\n",
    "As you can see, models have evolved from single-task predictions and standalone deployments, to models that perform multiple tasks, share model components, and\n",
    "are integrated to form a solution, such as in the healthcare document-handling and the sports broadcasting examples. Another way of describing these integrated model solutions is as a serving pipeline. A pipeline is made of connected components; the output of one component is the input to another, and each component is configurable, replaceable, and has version control and history. Using pipelines in today’s production ML extends across the entire end-to-end pipeline.\n",
    "\n",
    "## 3 The benefits of design patterns\n",
    "A design pattern implies that a best practice exists for constructing and coding a model that can be reapplied across a wide range of cases, such as image classification, object detection and tracking, facial recognition, image segmentation, super-resolution, and style transfer for image data; document classification, sentiment analysis, entity extraction, and summarization for text data; and classification, regression, and forecasting for unstructured data.\n",
    "\n",
    "The development of design patterns for deep learning is what led to model amalgamation, model fusion, and machine design—in which model components can be\n",
    "reused and adapted.\n",
    "\n",
    "These design patterns for model components allowed researchers and other deep learning practitioners to incrementally develop both model components and best practices for applications, across all models and data type\n",
    "\n",
    "This knowledge sharing accelerated the development of design patterns and the reuse of model components that makes it possible to deploy deep learning into widespread production applications.\n",
    "\n",
    "One of the earliest design patterns for neural network models was procedural reuse, which was simultaneously adopted across computer vision, NLU, and structured data. As with a software application, we design a procedural reuse model as components that reflect the data flow and decompose components into reusable functions.\n",
    "\n",
    "\n",
    "# Summary\n",
    "1. Deep learning evolved from classical AI to narrow AI, which led to using AI to solve problems with high-dimensionality inputs.\n",
    "2. Deep learning has evolved from experimenting with models to a reusable and reconfigurable pipeline approach for data, training, deployment, and serving.\n",
    "3. At the leading edge at the enterprise scale, ML practitioners are using model amalgamation, model fusion, and machine design.\n",
    "4. The procedural reuse design pattern is the building block and segue into today’s leading edge at the enterprise scale\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}